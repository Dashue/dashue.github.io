<rss xmlns:a10="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Johan Nilssons blog</title><link>http://www.johanilsson.com/feed.xml</link><description>Johan Nilssons blog</description><item><guid isPermaLink="true">http://www.johanilsson.com/2013/10/migrating-to-sandra-snow/</guid><link>http://www.johanilsson.com/2013/10/migrating-to-sandra-snow/</link><title>Migrating To Sandra.Snow</title><description>&lt;h1&gt;Migrating to Sandra.Snow&lt;/h1&gt;

&lt;p&gt;Having tried hosting my blog on tumblr, my own wordpress instance and wordpress online I've never been really happy. To me, writing should be simple and easy, so when I first heard of Sandra.Snow: A solution to freely host your blog on github pages with full control over everything, I jumped on it!&lt;/p&gt;

&lt;p&gt;First thing I had to do was migrate my original posts (html) off of wordpress. The other people using Sandra.Snow that I know of created a Wordpress export file and manually started to migrate posts over. I on the other hand claimed "Why spend 2 hours of manual labour when I can spend two weeks to automate the process". In hindsight not the best approach :) , but from it were two open source solutions born:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://www.nuget.org/packages/HtmlToMarkdown.Net/"&gt;Html To Markdown.Net&lt;/a&gt;  &lt;em&gt;(Apparently this didn´t exist already)&lt;/em&gt;&lt;br /&gt;
&lt;a href="https://github.com/Dashue/FromWordpressToSandraSnow"&gt;From Wordpress To Sandra.Snow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So in the following days I will continue to sanity-check old posts and publish them here.&lt;br /&gt;
&lt;a href="https://github.com/Sandra/Sandra.Snow"&gt;Fork from here&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://jabbr.net/#/rooms/SandraSnow"&gt;Questions goes here&lt;/a&gt;&lt;/p&gt;
</description><pubDate>Mon, 21 Oct 2013 04:00:00 Z</pubDate><a10:updated>2013-10-21T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;Migrating to Sandra.Snow&lt;/h1&gt;

&lt;p&gt;Having tried hosting my blog on tumblr, my own wordpress instance and wordpress online I've never been really happy. To me, writing should be simple and easy, so when I first heard of Sandra.Snow: A solution to freely host your blog on github pages with full control over everything, I jumped on it!&lt;/p&gt;

&lt;p&gt;First thing I had to do was migrate my original posts (html) off of wordpress. The other people using Sandra.Snow that I know of created a Wordpress export file and manually started to migrate posts over. I on the other hand claimed "Why spend 2 hours of manual labour when I can spend two weeks to automate the process". In hindsight not the best approach :) , but from it were two open source solutions born:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://www.nuget.org/packages/HtmlToMarkdown.Net/"&gt;Html To Markdown.Net&lt;/a&gt;  &lt;em&gt;(Apparently this didn´t exist already)&lt;/em&gt;&lt;br /&gt;
&lt;a href="https://github.com/Dashue/FromWordpressToSandraSnow"&gt;From Wordpress To Sandra.Snow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So in the following days I will continue to sanity-check old posts and publish them here.&lt;br /&gt;
&lt;a href="https://github.com/Sandra/Sandra.Snow"&gt;Fork from here&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://jabbr.net/#/rooms/SandraSnow"&gt;Questions goes here&lt;/a&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://www.johanilsson.com/2012/02/why-youd-want-to-place-a-shared-library-in-its-own/</guid><link>http://www.johanilsson.com/2012/02/why-youd-want-to-place-a-shared-library-in-its-own/</link><title>Why you'd want to place a shared library in its own solution</title><description>&lt;h1&gt;Where to place shared functionality&lt;/h1&gt;

&lt;p&gt;A situation occurred on work where we decided to refactor duplicated funtionality from two project (A and B) into a new shared library (X) DRYing things up. During this process the question of where to put X came up. I quickly answered "It's own solution" whereupon my colleague countered with the very simple yet valid question "Why" and I answered "Because" thinking it was really obvious and at the time couldn't understand why he'd even ask.&lt;/p&gt;

&lt;p&gt;After work I realized this was a childish and not a very helpful answer caused by me just knowing it was a good thing to do and have never reflected upon the reason for it. So I decided to gather my thoughts and write this blog giving a better answer to his valid question.&lt;/p&gt;

&lt;h3&gt;Presumptions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;We will use internal Nuget feed for the library.&lt;/li&gt;
&lt;li&gt;X could be placed in either A or B or separate library.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Consuming the Library&lt;/h3&gt;

&lt;p&gt;Placing it in its own solution forces all projects consume it the same way, otherwise some projects would use nuget and one could use project reference. &lt;/p&gt;

&lt;h3&gt;Future Changes&lt;/h3&gt;

&lt;p&gt;If placed in A it would be easy to change according to requirement or quick fix for A and not think about B or any future consumers. Placing it in its own solution forces it to behave more like an independent library and less like a project reference. It makes more sense to open the solution for the library when change is needed than to open an unrelated solution containing the library.&lt;/p&gt;

&lt;h3&gt;Unrelated Functionality&lt;/h3&gt;

&lt;p&gt;Since the functionality is general and not related to neither A nor B, placing it in the same solution just because it's consumed by them is not a valid argument.&lt;/p&gt;

&lt;h3&gt;Versioning&lt;/h3&gt;

&lt;p&gt;The library has different release/deploy cycles than the consumers and using Nuget allows for choosing what version and if/when to upgrade. Different consumers can easily consume different versions of the library. &lt;/p&gt;

&lt;h3&gt;YAGNI&lt;/h3&gt;

&lt;p&gt;YAGNI was mentioned as an argument against placing X in its own library. I think developers are to fast to throw abbreviations as arguments thinking they are valid without actually considering the situation at hand, but this is material for a future post.&lt;/p&gt;
</description><pubDate>Sat, 18 Feb 2012 04:00:00 Z</pubDate><a10:updated>2012-02-18T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;Where to place shared functionality&lt;/h1&gt;

&lt;p&gt;A situation occurred on work where we decided to refactor duplicated funtionality from two project (A and B) into a new shared library (X) DRYing things up. During this process the question of where to put X came up. I quickly answered "It's own solution" whereupon my colleague countered with the very simple yet valid question "Why" and I answered "Because" thinking it was really obvious and at the time couldn't understand why he'd even ask.&lt;/p&gt;

&lt;p&gt;After work I realized this was a childish and not a very helpful answer caused by me just knowing it was a good thing to do and have never reflected upon the reason for it. So I decided to gather my thoughts and write this blog giving a better answer to his valid question.&lt;/p&gt;

&lt;h3&gt;Presumptions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;We will use internal Nuget feed for the library.&lt;/li&gt;
&lt;li&gt;X could be placed in either A or B or separate library.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Consuming the Library&lt;/h3&gt;

&lt;p&gt;Placing it in its own solution forces all projects consume it the same way, otherwise some projects would use nuget and one could use project reference. &lt;/p&gt;

&lt;h3&gt;Future Changes&lt;/h3&gt;

&lt;p&gt;If placed in A it would be easy to change according to requirement or quick fix for A and not think about B or any future consumers. Placing it in its own solution forces it to behave more like an independent library and less like a project reference. It makes more sense to open the solution for the library when change is needed than to open an unrelated solution containing the library.&lt;/p&gt;

&lt;h3&gt;Unrelated Functionality&lt;/h3&gt;

&lt;p&gt;Since the functionality is general and not related to neither A nor B, placing it in the same solution just because it's consumed by them is not a valid argument.&lt;/p&gt;

&lt;h3&gt;Versioning&lt;/h3&gt;

&lt;p&gt;The library has different release/deploy cycles than the consumers and using Nuget allows for choosing what version and if/when to upgrade. Different consumers can easily consume different versions of the library. &lt;/p&gt;

&lt;h3&gt;YAGNI&lt;/h3&gt;

&lt;p&gt;YAGNI was mentioned as an argument against placing X in its own library. I think developers are to fast to throw abbreviations as arguments thinking they are valid without actually considering the situation at hand, but this is material for a future post.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://www.johanilsson.com/2012/01/photoutil-v1-0-0-1-aka-speeding-up-a-c-project/</guid><link>http://www.johanilsson.com/2012/01/photoutil-v1-0-0-1-aka-speeding-up-a-c-project/</link><title>PhotoUtil v1.0.0.1 aka Speeding up a c# project</title><description>&lt;h1&gt;Speeding up my hobby project&lt;/h1&gt;

&lt;p&gt;I was investigating why my hobby project &lt;a href="https://github.com/Dashue/MediaOrganizer"&gt;"PhotoUtil"&lt;/a&gt; was being slow.
A program that takes a folder with a bunch of photos as inputs, processes the metadata from them, more specifically the date the picture was taken and then organizes them in folders based on this.&lt;/p&gt;

&lt;p&gt;Following is a short recap of that procedure.&lt;/p&gt;

&lt;h3&gt;Find out what's taking time 1: Analyze / Launch Performance Wizard&lt;/h3&gt;

&lt;p&gt;The first thing that popped out at me was a DateTime creation which took 100% of the CPU time (according to the performance report). I grab the DateTimeOriginal value from exif metadata of a photo and convert it into datetime, only because it allowed for cleaner code when accessing the date and time values later on in the code. Apparently parsing strings into datetime is slow, so instead I got it to work with some string manipulation.&lt;/p&gt;

&lt;h3&gt;Find out what's taking time 2: Analyze / Launch Performance Wizard&lt;/h3&gt;

&lt;p&gt;When reading exif data using GDI+ from a file the whole image is read into memory using new Bitmap(Path); then the desired properties are fetched using GetProperty. This made me switch focus to increasing concurrent files being worked on (read parallelization) instead of minimizing process time of each file. With this in mind I rewrote my foreach loop into using the parallel library.&lt;/p&gt;

&lt;p&gt;from&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foreach(var filepath in filepaths)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;into&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Parallel.ForEach(filePaths, oldFilePath); 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Taking it one step further:&lt;/h3&gt;

&lt;p&gt;After finding out that the whole image is read into memory I set out to find an alternate approach of extracting EXIF data from a photo. It wasn't long before I found a sweet project called &lt;a href="http://www.codeproject.com/Articles/36342/ExifLib-A-Fast-Exif-Data-Extractor-for-NET-2-0"&gt;ExifLib&lt;/a&gt; on &lt;a href="https://www.nuget.org/packages/ExifLib/"&gt;Nuget&lt;/a&gt; which only reads the Exif data portion of an image. ExifLib is a wonderful lib created by Simon McKenzie only available as source code from codeproject.com but I'm hoping to convince him to put it up on NuGet so more people can find and make use of it.&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;p&gt;100 images processed on an Intel Core 2 duo&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Before optimizations:   00:00:20.8059121  
After optimizations:    00:00:11:4034511
Using ExifLib:          00:00:01.0714248
&lt;/code&gt;&lt;/pre&gt;
</description><pubDate>Wed, 25 Jan 2012 04:00:00 Z</pubDate><a10:updated>2012-01-25T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;Speeding up my hobby project&lt;/h1&gt;

&lt;p&gt;I was investigating why my hobby project &lt;a href="https://github.com/Dashue/MediaOrganizer"&gt;"PhotoUtil"&lt;/a&gt; was being slow.
A program that takes a folder with a bunch of photos as inputs, processes the metadata from them, more specifically the date the picture was taken and then organizes them in folders based on this.&lt;/p&gt;

&lt;p&gt;Following is a short recap of that procedure.&lt;/p&gt;

&lt;h3&gt;Find out what's taking time 1: Analyze / Launch Performance Wizard&lt;/h3&gt;

&lt;p&gt;The first thing that popped out at me was a DateTime creation which took 100% of the CPU time (according to the performance report). I grab the DateTimeOriginal value from exif metadata of a photo and convert it into datetime, only because it allowed for cleaner code when accessing the date and time values later on in the code. Apparently parsing strings into datetime is slow, so instead I got it to work with some string manipulation.&lt;/p&gt;

&lt;h3&gt;Find out what's taking time 2: Analyze / Launch Performance Wizard&lt;/h3&gt;

&lt;p&gt;When reading exif data using GDI+ from a file the whole image is read into memory using new Bitmap(Path); then the desired properties are fetched using GetProperty. This made me switch focus to increasing concurrent files being worked on (read parallelization) instead of minimizing process time of each file. With this in mind I rewrote my foreach loop into using the parallel library.&lt;/p&gt;

&lt;p&gt;from&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foreach(var filepath in filepaths)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;into&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Parallel.ForEach(filePaths, oldFilePath); 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Taking it one step further:&lt;/h3&gt;

&lt;p&gt;After finding out that the whole image is read into memory I set out to find an alternate approach of extracting EXIF data from a photo. It wasn't long before I found a sweet project called &lt;a href="http://www.codeproject.com/Articles/36342/ExifLib-A-Fast-Exif-Data-Extractor-for-NET-2-0"&gt;ExifLib&lt;/a&gt; on &lt;a href="https://www.nuget.org/packages/ExifLib/"&gt;Nuget&lt;/a&gt; which only reads the Exif data portion of an image. ExifLib is a wonderful lib created by Simon McKenzie only available as source code from codeproject.com but I'm hoping to convince him to put it up on NuGet so more people can find and make use of it.&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;p&gt;100 images processed on an Intel Core 2 duo&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Before optimizations:   00:00:20.8059121  
After optimizations:    00:00:11:4034511
Using ExifLib:          00:00:01.0714248
&lt;/code&gt;&lt;/pre&gt;
</a10:content></item><item><guid isPermaLink="true">http://www.johanilsson.com/2012/01/code-that-is-hard-to-be-covered-by-tests-is/</guid><link>http://www.johanilsson.com/2012/01/code-that-is-hard-to-be-covered-by-tests-is/</link><title>Hard tested code</title><description>&lt;h1&gt;Hard tested code&lt;/h1&gt;

&lt;p&gt;Code that is hard to be covered by tests is bug-prone and need to be redesigned to be easily coverable by easy tests.&lt;/p&gt;

&lt;p&gt;Writing a complete test suite naturally leads to good design, enforcing low coupling and high cohesion.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://codebetter.com/patricksmacchia/2012/01/10/non-trivial-and-real-world-feedbacks-on-writing-unit-tests"&gt;http://codebetter.com/patricksmacchia/2012/01/10/non-trivial-and-real-world-feedbacks-on-writing-unit-tests&lt;/a&gt;&lt;/p&gt;
</description><pubDate>Sat, 14 Jan 2012 04:00:00 Z</pubDate><a10:updated>2012-01-14T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;Hard tested code&lt;/h1&gt;

&lt;p&gt;Code that is hard to be covered by tests is bug-prone and need to be redesigned to be easily coverable by easy tests.&lt;/p&gt;

&lt;p&gt;Writing a complete test suite naturally leads to good design, enforcing low coupling and high cohesion.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://codebetter.com/patricksmacchia/2012/01/10/non-trivial-and-real-world-feedbacks-on-writing-unit-tests"&gt;http://codebetter.com/patricksmacchia/2012/01/10/non-trivial-and-real-world-feedbacks-on-writing-unit-tests&lt;/a&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://www.johanilsson.com/2012/01/review-tfs-11-the-things-i-look-forward-to/</guid><link>http://www.johanilsson.com/2012/01/review-tfs-11-the-things-i-look-forward-to/</link><title>Review TFS 11: The things I look forward to</title><description>&lt;h1&gt;Review TFS 11: The things I look forward to&lt;/h1&gt;

&lt;p&gt;With TFS 11 around the corner (with in-place upgrade from 2010), having been rewritten with the promise of a faster and smarter experience I felt like compiling a list of the features I look forward to the most. Ordered by pain factor solving desc.&lt;/p&gt;

&lt;h3&gt;Automerge content changes (finally)&lt;/h3&gt;

&lt;p&gt;Finally TFS has come to it&amp;#8217;s senses and allows for automerge of non-conflicting content changes. &lt;/p&gt;

&lt;h3&gt;My work pane and Search work items&lt;/h3&gt;

&lt;p&gt;No need for custom work item queries, work related to you will be easily available in the work item pane. Searching for items is also available here.&lt;/p&gt;

&lt;h3&gt;Easier to transition between work items states&lt;/h3&gt;

&lt;p&gt;No more open work item and changing state to in progress, just right-click item in my work pane and click start.&lt;/p&gt;

&lt;h3&gt;Suspending&lt;/h3&gt;

&lt;p&gt;Easier task switching by using suspend/resume. Saves visual studio state, open files and position in currently open file. Suspended tasks are listed in you work pane.&lt;/p&gt;

&lt;h3&gt;New Pending changes page&lt;/h3&gt;

&lt;p&gt;Shows tree of pending changes. Allows or leaving files in excluded state without having to exclude changes every check-in (good for personalized web.config etc.).&lt;/p&gt;

&lt;h3&gt;New built-in diff tool&lt;/h3&gt;

&lt;p&gt;TFS team has finally gotten around to fix the built in diff tool, which now supports more view modes and reflects live changes. Which means diff will update if you make changes to your version when showing base/yours. The merge tool has also gotten some love.&lt;/p&gt;

&lt;h3&gt;Local workspaces&lt;/h3&gt;

&lt;p&gt;Removes offline mode, and the time out method require to go offline. Doesn&amp;#8217;t handle modified files by reading the read-only bit on the file (Took them until 2012 to fix it). File system watcher watch for change made outside of VS, and lists them nicely in the pending changes pane.&lt;/p&gt;

&lt;h3&gt;Shelving&lt;/h3&gt;

&lt;p&gt;Shelveset merge (with current and baseless) and searching of shelvesets.&lt;/p&gt;

&lt;h3&gt;Code Review workflow&lt;/h3&gt;

&lt;p&gt;Request code-review workflow on suspended changeset or checked in changeset as well as code reviewing functionality with the ability to comment on specific changes and even alter the changeset.&lt;/p&gt;

&lt;h3&gt;Rollback changesets&lt;/h3&gt;

&lt;p&gt;Awesome&lt;/p&gt;

&lt;h3&gt;Async operations&lt;/h3&gt;

&lt;p&gt;Modality and blocking operations has now been removed, no more waiting on visual studio to complete retrieving things (search items etc.).&lt;/p&gt;

&lt;h3&gt;New Builds page&lt;/h3&gt;

&lt;p&gt;Shows builds associated to me (started by me etc.). Extensible&lt;/p&gt;

&lt;h3&gt;Summary:&lt;/h3&gt;

&lt;p&gt;I really like the new way that TFS 11 shows data; putting the concept of "My work" in focus, showing my work items and my builds etc. This combined with the new functionalities provided, working in TFS 11 will be much more smother and faster than it's predecessors and I am really looking forward til it's released and part of my development environment.&lt;/p&gt;

&lt;p&gt;Reference: &lt;a href="http://channel9.msdn.com/Events/BUILD/BUILD2011/TOOL-811T"&gt;Developer collaboration with Team Foundation Server 11&lt;/a&gt;&lt;/p&gt;
</description><pubDate>Fri, 13 Jan 2012 04:00:00 Z</pubDate><a10:updated>2012-01-13T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;Review TFS 11: The things I look forward to&lt;/h1&gt;

&lt;p&gt;With TFS 11 around the corner (with in-place upgrade from 2010), having been rewritten with the promise of a faster and smarter experience I felt like compiling a list of the features I look forward to the most. Ordered by pain factor solving desc.&lt;/p&gt;

&lt;h3&gt;Automerge content changes (finally)&lt;/h3&gt;

&lt;p&gt;Finally TFS has come to it&amp;#8217;s senses and allows for automerge of non-conflicting content changes. &lt;/p&gt;

&lt;h3&gt;My work pane and Search work items&lt;/h3&gt;

&lt;p&gt;No need for custom work item queries, work related to you will be easily available in the work item pane. Searching for items is also available here.&lt;/p&gt;

&lt;h3&gt;Easier to transition between work items states&lt;/h3&gt;

&lt;p&gt;No more open work item and changing state to in progress, just right-click item in my work pane and click start.&lt;/p&gt;

&lt;h3&gt;Suspending&lt;/h3&gt;

&lt;p&gt;Easier task switching by using suspend/resume. Saves visual studio state, open files and position in currently open file. Suspended tasks are listed in you work pane.&lt;/p&gt;

&lt;h3&gt;New Pending changes page&lt;/h3&gt;

&lt;p&gt;Shows tree of pending changes. Allows or leaving files in excluded state without having to exclude changes every check-in (good for personalized web.config etc.).&lt;/p&gt;

&lt;h3&gt;New built-in diff tool&lt;/h3&gt;

&lt;p&gt;TFS team has finally gotten around to fix the built in diff tool, which now supports more view modes and reflects live changes. Which means diff will update if you make changes to your version when showing base/yours. The merge tool has also gotten some love.&lt;/p&gt;

&lt;h3&gt;Local workspaces&lt;/h3&gt;

&lt;p&gt;Removes offline mode, and the time out method require to go offline. Doesn&amp;#8217;t handle modified files by reading the read-only bit on the file (Took them until 2012 to fix it). File system watcher watch for change made outside of VS, and lists them nicely in the pending changes pane.&lt;/p&gt;

&lt;h3&gt;Shelving&lt;/h3&gt;

&lt;p&gt;Shelveset merge (with current and baseless) and searching of shelvesets.&lt;/p&gt;

&lt;h3&gt;Code Review workflow&lt;/h3&gt;

&lt;p&gt;Request code-review workflow on suspended changeset or checked in changeset as well as code reviewing functionality with the ability to comment on specific changes and even alter the changeset.&lt;/p&gt;

&lt;h3&gt;Rollback changesets&lt;/h3&gt;

&lt;p&gt;Awesome&lt;/p&gt;

&lt;h3&gt;Async operations&lt;/h3&gt;

&lt;p&gt;Modality and blocking operations has now been removed, no more waiting on visual studio to complete retrieving things (search items etc.).&lt;/p&gt;

&lt;h3&gt;New Builds page&lt;/h3&gt;

&lt;p&gt;Shows builds associated to me (started by me etc.). Extensible&lt;/p&gt;

&lt;h3&gt;Summary:&lt;/h3&gt;

&lt;p&gt;I really like the new way that TFS 11 shows data; putting the concept of "My work" in focus, showing my work items and my builds etc. This combined with the new functionalities provided, working in TFS 11 will be much more smother and faster than it's predecessors and I am really looking forward til it's released and part of my development environment.&lt;/p&gt;

&lt;p&gt;Reference: &lt;a href="http://channel9.msdn.com/Events/BUILD/BUILD2011/TOOL-811T"&gt;Developer collaboration with Team Foundation Server 11&lt;/a&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://www.johanilsson.com/2012/01/enumerable-zip/</guid><link>http://www.johanilsson.com/2012/01/enumerable-zip/</link><title>Enumerable.Zip</title><description>&lt;h1&gt;Enumerable.Zip&lt;/h1&gt;

&lt;p&gt;I just stumbled upon Enumerable.Zip which is a newly added Linq operator since .NET Framework 4.&lt;/p&gt;

&lt;p&gt;Signature is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static IEnumerable&amp;lt;TResult&amp;gt; Zip&amp;lt;TFirst, TSecond, TResult&amp;gt;(
    this IEnumerable&amp;lt;TFirst&amp;gt; first,
    IEnumerable&amp;lt;TSecond&amp;gt; second,
    Func&amp;lt;TFirst, TSecond, TResult&amp;gt; func);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much like a zipper it "zips" together two IEnumerables applying a specified function over each pair of elements.
I've seen examples which print the greater number from two list of ints, which constructs addresses from four lists of city/street/number/flatnumber and one that prints name and age from name/age lists.&lt;/p&gt;

&lt;p&gt;I have as of yet found a good real world example, and am incapable of thinking one up.&lt;br /&gt;
Anyone out there who have made good use of this added functionality?&lt;/p&gt;
</description><pubDate>Thu, 12 Jan 2012 04:00:00 Z</pubDate><a10:updated>2012-01-12T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;Enumerable.Zip&lt;/h1&gt;

&lt;p&gt;I just stumbled upon Enumerable.Zip which is a newly added Linq operator since .NET Framework 4.&lt;/p&gt;

&lt;p&gt;Signature is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public static IEnumerable&amp;lt;TResult&amp;gt; Zip&amp;lt;TFirst, TSecond, TResult&amp;gt;(
    this IEnumerable&amp;lt;TFirst&amp;gt; first,
    IEnumerable&amp;lt;TSecond&amp;gt; second,
    Func&amp;lt;TFirst, TSecond, TResult&amp;gt; func);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much like a zipper it "zips" together two IEnumerables applying a specified function over each pair of elements.
I've seen examples which print the greater number from two list of ints, which constructs addresses from four lists of city/street/number/flatnumber and one that prints name and age from name/age lists.&lt;/p&gt;

&lt;p&gt;I have as of yet found a good real world example, and am incapable of thinking one up.&lt;br /&gt;
Anyone out there who have made good use of this added functionality?&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://www.johanilsson.com/2012/01/my-take-on-ref/</guid><link>http://www.johanilsson.com/2012/01/my-take-on-ref/</link><title>My take on Ref</title><description>&lt;h1&gt;My Take on Ref&lt;/h1&gt;

&lt;p&gt;A good friend of mine asked me to share my thoughts on the ref keyword so here goes.&lt;/p&gt;

&lt;p&gt;To give it some context: To me a good method is a function taking a variable amount of parameters, making some processing and possibly returning the result. Furthermore the method should do one thing and one thing only and never change input parameter values.&lt;/p&gt;

&lt;p&gt;Now the ref keyword allow developers to stray from this "ideal" method by allowing it to change the values of the input parameters. Obfuscating the self-documentation of the method and by this diffusing what the method does and what it's single responsibility is. This creates a required understanding by your fellow colleagues about the intricates of the method, and in turn requires every developer to know how every method in the system is implemented.&lt;/p&gt;

&lt;p&gt;The most common reason for using ref parameters is simply that your method is doing to much, rendering you unable to return a single value/collection to account for everything that is being done, ending up with sending and object as input parameter and changing a lot of values. This may be improved by returning part of the data as an out parameter and part of it as the return value if possible.&lt;/p&gt;

&lt;p&gt;The reason why I think that using out is better than ref is because out doesn't obfuscate the api and the intention of the method. Using Ref conveys that "This method might(?) change values of the parameter or even the parameter itself" while Out conveys that "This method will change the parameter".&lt;/p&gt;

&lt;p&gt;Consider these two implementations of this contrived example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Example 1: Returning value
Assert.AreEqual(4, Square(2));

Example 2: Using Ref
float a = 2;  
Square(a);  
Assert.AreEqual(4, a);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I would like to end this post with my oppinion on the "Ref" and "Out" keywords.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Although there are valid cases when the right solution is to use Ref and Out, from my experince I say you rarely need to use Out, and you need a really good reason for using Ref.&lt;/p&gt;
&lt;/blockquote&gt;
</description><pubDate>Wed, 11 Jan 2012 04:00:00 Z</pubDate><a10:updated>2012-01-11T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;My Take on Ref&lt;/h1&gt;

&lt;p&gt;A good friend of mine asked me to share my thoughts on the ref keyword so here goes.&lt;/p&gt;

&lt;p&gt;To give it some context: To me a good method is a function taking a variable amount of parameters, making some processing and possibly returning the result. Furthermore the method should do one thing and one thing only and never change input parameter values.&lt;/p&gt;

&lt;p&gt;Now the ref keyword allow developers to stray from this "ideal" method by allowing it to change the values of the input parameters. Obfuscating the self-documentation of the method and by this diffusing what the method does and what it's single responsibility is. This creates a required understanding by your fellow colleagues about the intricates of the method, and in turn requires every developer to know how every method in the system is implemented.&lt;/p&gt;

&lt;p&gt;The most common reason for using ref parameters is simply that your method is doing to much, rendering you unable to return a single value/collection to account for everything that is being done, ending up with sending and object as input parameter and changing a lot of values. This may be improved by returning part of the data as an out parameter and part of it as the return value if possible.&lt;/p&gt;

&lt;p&gt;The reason why I think that using out is better than ref is because out doesn't obfuscate the api and the intention of the method. Using Ref conveys that "This method might(?) change values of the parameter or even the parameter itself" while Out conveys that "This method will change the parameter".&lt;/p&gt;

&lt;p&gt;Consider these two implementations of this contrived example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Example 1: Returning value
Assert.AreEqual(4, Square(2));

Example 2: Using Ref
float a = 2;  
Square(a);  
Assert.AreEqual(4, a);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I would like to end this post with my oppinion on the "Ref" and "Out" keywords.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Although there are valid cases when the right solution is to use Ref and Out, from my experince I say you rarely need to use Out, and you need a really good reason for using Ref.&lt;/p&gt;
&lt;/blockquote&gt;
</a10:content></item><item><guid isPermaLink="true">http://www.johanilsson.com/2011/12/learning-steps-for-design-patterns-from-dnrtv-194/</guid><link>http://www.johanilsson.com/2011/12/learning-steps-for-design-patterns-from-dnrtv-194/</link><title>The 4 stages of learning a design pattern</title><description>&lt;h1&gt;Stages of Learning a Design Pattern&lt;/h1&gt;

&lt;h3&gt;Stage 0 - Ignorance&lt;/h3&gt;

&lt;p&gt;You used what? Never heard of it.&lt;/p&gt;

&lt;h3&gt;Stage 1 - Awakening&lt;/h3&gt;

&lt;p&gt;Wow, I just learned how XYZ pattern can improve my design. I'm not really sure where it would work in my code, but I'm definitely looking.&lt;/p&gt;

&lt;h3&gt;Stage 2 - Overzealous&lt;/h3&gt;

&lt;p&gt;I totally "get" the XYZ pattern; I'm adding it everywhere I can shoehorn it into m code. My design's gonna be better now, for sure!&lt;/p&gt;

&lt;h3&gt;Stage 3 - Mastery&lt;/h3&gt;

&lt;p&gt;After allowing the design to evolve throuhg interaction with the users and the addition of tests, it started to exhibit ABC negative characteristics, but the XYZ pattern was a logical next step and was achieved through a simple refactoring.   &lt;/p&gt;

&lt;p&gt;Heard on &lt;a href="http://www.dnrtv.com/default.aspx?showNum=194"&gt;dnrTV 194 Steve Smith on Commonly Used Design Patterns&lt;/a&gt;&lt;/p&gt;
</description><pubDate>Fri, 09 Dec 2011 04:00:00 Z</pubDate><a10:updated>2011-12-09T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;Stages of Learning a Design Pattern&lt;/h1&gt;

&lt;h3&gt;Stage 0 - Ignorance&lt;/h3&gt;

&lt;p&gt;You used what? Never heard of it.&lt;/p&gt;

&lt;h3&gt;Stage 1 - Awakening&lt;/h3&gt;

&lt;p&gt;Wow, I just learned how XYZ pattern can improve my design. I'm not really sure where it would work in my code, but I'm definitely looking.&lt;/p&gt;

&lt;h3&gt;Stage 2 - Overzealous&lt;/h3&gt;

&lt;p&gt;I totally "get" the XYZ pattern; I'm adding it everywhere I can shoehorn it into m code. My design's gonna be better now, for sure!&lt;/p&gt;

&lt;h3&gt;Stage 3 - Mastery&lt;/h3&gt;

&lt;p&gt;After allowing the design to evolve throuhg interaction with the users and the addition of tests, it started to exhibit ABC negative characteristics, but the XYZ pattern was a logical next step and was achieved through a simple refactoring.   &lt;/p&gt;

&lt;p&gt;Heard on &lt;a href="http://www.dnrtv.com/default.aspx?showNum=194"&gt;dnrTV 194 Steve Smith on Commonly Used Design Patterns&lt;/a&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://www.johanilsson.com/2011/11/daily-refactoring/</guid><link>http://www.johanilsson.com/2011/11/daily-refactoring/</link><title>Daily refactoring</title><description>&lt;h1&gt;Daily Refactoring&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;#region MyRegion
    if (!MyHelper.MyBool)
    {
        myclass.Method(int, false, string, enum);
        MyHelper.MyBool = true;
    }
    else
    {
        myclass.Method(int, true, string, enum);
    }    
#endregion    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Is semantically the same as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;myClass.Method(int, MyHelper.MyBool, string, enum);
MyHelper.MyBool = true;      
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tidier and faster : )&lt;/p&gt;
</description><pubDate>Tue, 22 Nov 2011 04:00:00 Z</pubDate><a10:updated>2011-11-22T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;Daily Refactoring&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;#region MyRegion
    if (!MyHelper.MyBool)
    {
        myclass.Method(int, false, string, enum);
        MyHelper.MyBool = true;
    }
    else
    {
        myclass.Method(int, true, string, enum);
    }    
#endregion    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Is semantically the same as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;myClass.Method(int, MyHelper.MyBool, string, enum);
MyHelper.MyBool = true;      
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tidier and faster : )&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://www.johanilsson.com/2011/11/technical-debt-101/</guid><link>http://www.johanilsson.com/2011/11/technical-debt-101/</link><title>Technical debt 101</title><description>&lt;h1&gt;Technical Debt 101&lt;/h1&gt;

&lt;p&gt;I would like to start this article with the definition of what technical debt is. Technical debt is taking shortcuts for good or bad, for known or unknown reasons. The technical debt grows exponentially over time because of interest. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interest:&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Every minute spent on not-quite-right code counts as interest on that debt - Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The amount of interest is based on a number of things, among others:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Lack of knowledge - Either from time passing and people forget stuff, or from people changing jobs, leave of absence, death etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Increase in complexity - New functionality will be designed and implemented on top and according to existing technical debt, writing more code to get around code, with more code you need more tests and this code and tests needs review. Hence technical debt will increase technical debt.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How to spot debt:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are some signs to look for when you go "debt spotting&amp;amp;" to make it easier to point technical debt.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The code areas where you as a developer when assigned bugs or to extend this code feel your heart sink, your moral drop e.g spaghetti code, or unnecessary complexity, or if the intent of the code isn't clear enough so it has to use comments or regions as a crutch for relaying what it does&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The average amounts of tasks per sprint decreases&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The number of tests increases at the same time as "average amount of tasks/sprint decreases"&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With an agile project methodology you don&amp;#8217;t necessarily have a lower technical debt but because of its support for rapid changes the debt will be highlighted at an earlier stage allowing you to decide if to pay this debt or not.&lt;/p&gt;

&lt;p&gt;In my experience test driven development and tests in general protect you from some technical debt because you've had to go through the steps of "red", "green", "refactor"; and thereby paid a little of the debt up front.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Paying debt:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Like with any debt there is always an enforcer, a collector that will &lt;em&gt;make&lt;/em&gt; you pay that debt. With technical debt, this collector is usually time, or new requirements.  &lt;/p&gt;

&lt;p&gt;Because of the interest, paying the debt will be more and more expensive as time goes, until you are forced by time to pay, at its most latest stage. This is when its the most expensive to pay the debt because you are most prone to the interests of people with skills have moved on, system have changed, a lot of technical debt has been built and made dependent on underlying technical debt. The debt may even be so large so for you to be able to pay it your feature development will stagnate and may even lead to loss of market pieces.&lt;/p&gt;

&lt;p&gt;Often project managers don't allow developers to pay technical debt, or don't prioritise it since its sometimes hard to motivate a sprint that starts with 12 tasks and ends with 12 tasks still to do, with the only thing the developers salary has paid is the debt.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Types of Technical Debt:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bugs are not technical debt until someone decides to not fix them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Manage technical debt = planned technical debt&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Unmanaged is the worst, this is only fixed when the pain is felt&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Monitoring Technical Debt:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to be able to monitor the debt, you must keep track of it. This involves keeping track of both the managed debt introduced on purpose using shortcuts, quick solutions and the "not-quite-right" solutions, and the unmanaged ones whenever they are encountered and grade them in some way that gives them a relational size difference. My personal view is that for every day that goes by of coding, it's very unlikely that the technical debt remains the same but instead rather if you don't pay some you gain some. Taking time into consideration, just a small daily increase will eventually become very expensive to pay off, leave it too long and a rewrite may end up being your only solution. &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Technical debt is like an iceberg with only 10 percent visible.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Until next time, keep track of that 10 percent for all your life's worth : )&lt;/p&gt;
</description><pubDate>Thu, 10 Nov 2011 04:00:00 Z</pubDate><a10:updated>2011-11-10T04:00:00Z</a10:updated><a10:content type="text">&lt;h1&gt;Technical Debt 101&lt;/h1&gt;

&lt;p&gt;I would like to start this article with the definition of what technical debt is. Technical debt is taking shortcuts for good or bad, for known or unknown reasons. The technical debt grows exponentially over time because of interest. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interest:&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Every minute spent on not-quite-right code counts as interest on that debt - Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The amount of interest is based on a number of things, among others:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Lack of knowledge - Either from time passing and people forget stuff, or from people changing jobs, leave of absence, death etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Increase in complexity - New functionality will be designed and implemented on top and according to existing technical debt, writing more code to get around code, with more code you need more tests and this code and tests needs review. Hence technical debt will increase technical debt.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How to spot debt:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are some signs to look for when you go "debt spotting&amp;amp;" to make it easier to point technical debt.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The code areas where you as a developer when assigned bugs or to extend this code feel your heart sink, your moral drop e.g spaghetti code, or unnecessary complexity, or if the intent of the code isn't clear enough so it has to use comments or regions as a crutch for relaying what it does&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The average amounts of tasks per sprint decreases&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The number of tests increases at the same time as "average amount of tasks/sprint decreases"&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With an agile project methodology you don&amp;#8217;t necessarily have a lower technical debt but because of its support for rapid changes the debt will be highlighted at an earlier stage allowing you to decide if to pay this debt or not.&lt;/p&gt;

&lt;p&gt;In my experience test driven development and tests in general protect you from some technical debt because you've had to go through the steps of "red", "green", "refactor"; and thereby paid a little of the debt up front.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Paying debt:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Like with any debt there is always an enforcer, a collector that will &lt;em&gt;make&lt;/em&gt; you pay that debt. With technical debt, this collector is usually time, or new requirements.  &lt;/p&gt;

&lt;p&gt;Because of the interest, paying the debt will be more and more expensive as time goes, until you are forced by time to pay, at its most latest stage. This is when its the most expensive to pay the debt because you are most prone to the interests of people with skills have moved on, system have changed, a lot of technical debt has been built and made dependent on underlying technical debt. The debt may even be so large so for you to be able to pay it your feature development will stagnate and may even lead to loss of market pieces.&lt;/p&gt;

&lt;p&gt;Often project managers don't allow developers to pay technical debt, or don't prioritise it since its sometimes hard to motivate a sprint that starts with 12 tasks and ends with 12 tasks still to do, with the only thing the developers salary has paid is the debt.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Types of Technical Debt:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bugs are not technical debt until someone decides to not fix them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Manage technical debt = planned technical debt&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Unmanaged is the worst, this is only fixed when the pain is felt&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Monitoring Technical Debt:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to be able to monitor the debt, you must keep track of it. This involves keeping track of both the managed debt introduced on purpose using shortcuts, quick solutions and the "not-quite-right" solutions, and the unmanaged ones whenever they are encountered and grade them in some way that gives them a relational size difference. My personal view is that for every day that goes by of coding, it's very unlikely that the technical debt remains the same but instead rather if you don't pay some you gain some. Taking time into consideration, just a small daily increase will eventually become very expensive to pay off, leave it too long and a rewrite may end up being your only solution. &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Technical debt is like an iceberg with only 10 percent visible.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Until next time, keep track of that 10 percent for all your life's worth : )&lt;/p&gt;
</a10:content></item></channel></rss>